<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.4 Continuous random variables: An example using the Normal distribution | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.13.2 and GitBook 2.6.7" />

  <meta property="og:title" content="1.4 Continuous random variables: An example using the Normal distribution | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://bookdown.org/yihui/bookdown/" />
  <meta property="og:image" content="https://bookdown.org/yihui/bookdown/images/cover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="rstudio/bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.4 Continuous random variables: An example using the Normal distribution | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://bookdown.org/yihui/bookdown/images/cover.jpg" />

<meta name="author" content="Shravan Vasishth, Bruno Nicenboim, and Daniel Schad" />


<meta name="date" content="2019-10-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="discrete-random-variables-an-example-using-the-binomial-distribution.html"/>
<link rel="next" href="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Authoring Books with R Markdown</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>0.1</b> Prerequisites</a></li>
<li class="chapter" data-level="0.2" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i><b>0.2</b> How to read this book</a></li>
<li class="chapter" data-level="0.3" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i><b>0.3</b> Online materials</a></li>
<li class="chapter" data-level="0.4" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i><b>0.4</b> Software needed</a></li>
<li class="chapter" data-level="0.5" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>0.5</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introprob.html"><a href="introprob.html"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html"><i class="fa fa-check"></i><b>1.3</b> Discrete random variables: An example using the Binomial distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.3.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.3.2" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.3.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.4</b> Continuous random variables: An example using the Normal distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.4.1</b> An important distinction: probability vs.Â density in a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><a href="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><i class="fa fa-check"></i><b>1.5</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.6" data-path="summary-of-useful-r-functions-relating-to-distributions.html"><a href="summary-of-useful-r-functions-relating-to-distributions.html"><i class="fa fa-check"></i><b>1.6</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.7" data-path="summary-of-concepts-introduced-in-this-chapter.html"><a href="summary-of-concepts-introduced-in-this-chapter.html"><i class="fa fa-check"></i><b>1.7</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="1.8" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.8</b> Further reading</a></li>
<li class="chapter" data-level="1.9" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.9</b> Exercises</a><ul>
<li class="chapter" data-level="1.9.1" data-path="exercises.html"><a href="exercises.html#practice-using-the-pnorm-function"><i class="fa fa-check"></i><b>1.9.1</b> Practice using the <code>pnorm</code> function</a></li>
<li class="chapter" data-level="1.9.2" data-path="exercises.html"><a href="exercises.html#practice-using-the-qnorm-function"><i class="fa fa-check"></i><b>1.9.2</b> Practice using the <code>qnorm</code> function</a></li>
<li class="chapter" data-level="1.9.3" data-path="exercises.html"><a href="exercises.html#practice-using-qt"><i class="fa fa-check"></i><b>1.9.3</b> Practice using <code>qt</code></a></li>
<li class="chapter" data-level="1.9.4" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>1.9.4</b> Maximum likelihood estimation 1</a></li>
<li class="chapter" data-level="1.9.5" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>1.9.5</b> Maximum likelihood estimation 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introBDA.html"><a href="introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="sec-analytical.html"><a href="sec-analytical.html"><i class="fa fa-check"></i><b>2.1</b> Deriving the posterior using Bayesâ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.1.1" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.1.2" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-prior-for-theta"><i class="fa fa-check"></i><b>2.1.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="sec-analytical.html"><a href="sec-analytical.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.1.3</b> Using Bayesâ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="sec-analytical.html"><a href="sec-analytical.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.1.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.1.5" data-path="sec-analytical.html"><a href="sec-analytical.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.1.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.1.6" data-path="sec-analytical.html"><a href="sec-analytical.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.1.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.1.7" data-path="sec-analytical.html"><a href="sec-analytical.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.1.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="summary-of-concepts-introduced-in-this-chapter-1.html"><a href="summary-of-concepts-introduced-in-this-chapter-1.html"><i class="fa fa-check"></i><b>2.2</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="2.3" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>2.3</b> Further reading</a></li>
<li class="chapter" data-level="2.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="2.4.1" data-path="exercises-1.html"><a href="exercises-1.html#deriving-bayes-rule"><i class="fa fa-check"></i><b>2.4.1</b> Deriving Bayesâ rule</a></li>
<li class="chapter" data-level="2.4.2" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-1"><i class="fa fa-check"></i><b>2.4.2</b> Conjugate forms 1</a></li>
<li class="chapter" data-level="2.4.3" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-2"><i class="fa fa-check"></i><b>2.4.3</b> Conjugate forms 2</a></li>
<li class="chapter" data-level="2.4.4" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-3"><i class="fa fa-check"></i><b>2.4.4</b> Conjugate forms 3</a></li>
<li class="chapter" data-level="2.4.5" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-4"><i class="fa fa-check"></i><b>2.4.5</b> Conjugate forms 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="important-distributions.html"><a href="important-distributions.html"><i class="fa fa-check"></i><b>3</b> Important distributions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="continuous-random-variables-an-example-using-the-normal-distribution" class="section level2">
<h2><span class="header-section-number">1.4</span> Continuous random variables: An example using the Normal distribution</h2>
<p>We will now revisit the idea of the random variable using a continuous distribution. Imagine that you have a vector of reading time data <span class="math inline">\(y\)</span> measured in milliseconds and coming from a Normal distribution. The Normal distribution is defined in terms of two parameters: a mean value <span class="math inline">\(\mu\)</span>, which determines its center, and the variance <span class="math inline">\(\sigma^2\)</span>, which determines how much spread there is around this center point.</p>
<p>The probability density function (PDF) of the Normal distribution is defined as follows:</p>
<p><span class="math display">\[\begin{equation}
Normal(y|\mu,\sigma)=f(y)= \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left(-\frac{(y-\mu)^2}{2\sigma^2} \right)
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(\mu\)</span> is some true, unknown mean, and <span class="math inline">\(\sigma^2\)</span> is some true, unknown variance of the Normal distribution that the reading times have been sampled from. There is a built-in function in R that computes the above function once we specify the mean <span class="math inline">\(\mu\)</span> and the standard deviation <span class="math inline">\(\sigma\)</span> (in R, this parameter is specified in terms of the standard deviation rather than the variance).</p>
<p>Figure <a href="continuous-random-variables-an-example-using-the-normal-distribution.html#fig:normdistrn">1.4</a> visualizes the Normal distribution for particular values of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, as a PDF (using <code>dnorm</code>), a CDF (using <code>pnorm</code>), and the inverse CDF (using <code>qnorm</code>). It is clear from the figure that these are three different ways of looking at the same information.</p>
<div class="figure"><span id="fig:normdistrn"></span>
<img src="bookdown_files/figure-html/normdistrn-1.svg" alt="The PDF, CDF, and inverse CDF for the $Normal(\mu=500,\sigma=100)$." width="672" />
<p class="caption">
FIGURE 1.4: The PDF, CDF, and inverse CDF for the <span class="math inline">\(Normal(\mu=500,\sigma=100)\)</span>.
</p>
</div>

<div class="rmdnote">
to-do: Maybe this is the place to mention some interesting properties like:
Normal(mu, sigma) = mu + Normal(0,1) * sigma
(Weâll use this property a lot later when we code in Stan)
</div>

<p>As in the discrete example, the PDF, CDF, and inverse of the CDF allow us to ask questions like:</p>
<ul>
<li><strong>What is the probability of observing values between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> from a Normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span></strong>? Using the above example, we can ask what the probability of observing values between 200 and 700 ms:</li>
</ul>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1"><span class="kw">pnorm</span>(<span class="dv">700</span>,<span class="dt">mean=</span><span class="dv">500</span>,<span class="dt">sd=</span><span class="dv">100</span>)<span class="op">-</span><span class="kw">pnorm</span>(<span class="dv">200</span>,<span class="dt">mean=</span><span class="dv">500</span>,<span class="dt">sd=</span><span class="dv">100</span>)</a></code></pre></div>
<pre><code>## [1] 0.9759</code></pre>

<div class="rmdnote">
to-do: add figure illustrating the above
</div>

<p>Notice here that the probability of any point value in a PDF is always <span class="math inline">\(0\)</span>. This is because the probability in a continuous probability distribution is the area under the curve, and the area at any point on the x-axis is always <span class="math inline">\(0\)</span>. The implication here is that we can only ask about probabilities between two different points; e.g., the probability that <span class="math inline">\(Y\)</span> lies between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, or <span class="math inline">\(P(a&lt;Y&lt;b)\)</span>.</p>
<ul>
<li><strong>What is the quantile <span class="math inline">\(q\)</span> such that the probability is <span class="math inline">\(p\)</span> of observing that value <span class="math inline">\(q\)</span> or something less (or more) than it</strong>? For example, we can work out the quantile <span class="math inline">\(q\)</span> such that the probability of observing <span class="math inline">\(q\)</span> or something less than it is 0.975, in the Normal(500,100) distribution. Formally, we would write this as <span class="math inline">\(P(Y&lt;a)\)</span>.</li>
</ul>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb25-1" data-line-number="1"><span class="kw">qnorm</span>(<span class="fl">0.975</span>,<span class="dt">mean=</span><span class="dv">500</span>,<span class="dt">sd=</span><span class="dv">100</span>)</a></code></pre></div>
<pre><code>## [1] 696</code></pre>
<p>The above output says that the probability that the random variable is less than <span class="math inline">\(q=695\)</span> is 97.5%.</p>
<ul>
<li><strong>Generating simulated data</strong>. Given a vector of <span class="math inline">\(n\)</span> independent and identically distributed data <span class="math inline">\(y\)</span>, i.e., given that each data point is being generated independently from <span class="math inline">\(Y \sim Normal(\mu,\sigma)\)</span> for some values of the parameters, the maximum likelihood estimates for the expectation and variance<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> are:</li>
</ul>
<p><span class="math display">\[\begin{equation}
\bar{y} =  \frac{\sum_{i=1}^n y_i}{n} 
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
Var(y) = \frac{\sum_{i=1}^n (y_i-
\bar{y})^2}{n}
\end{equation}\]</span></p>
<p>For example, we can generate <span class="math inline">\(10\)</span> data points using the <code>rnorm</code> function, and then compute the mean and variance from the simulated data:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" data-line-number="1">y&lt;-<span class="kw">rnorm</span>(<span class="dv">10</span>,<span class="dt">mean=</span><span class="dv">500</span>,<span class="dt">sd=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb27-2" data-line-number="2"><span class="kw">mean</span>(y);<span class="kw">var</span>(y)</a></code></pre></div>
<pre><code>## [1] 559.6</code></pre>
<pre><code>## [1] 6778</code></pre>
<p>Again, the sample mean and sample variance computed from a particular data-set need not necessarily be close to the true values of the respective parameters.</p>
<div id="an-important-distinction-probability-vs.density-in-a-continuous-random-variable" class="section level3">
<h3><span class="header-section-number">1.4.1</span> An important distinction: probability vs.Â density in a continuous random variable</h3>
<p>In continuous distributions like the Normal discussed above, it is important to understand that the probability density function or PDF, <span class="math inline">\(p(y| \mu, \sigma)\)</span> defines a mapping from the <span class="math inline">\(y\)</span> values (the possible values that the data can have) to a quantity called the density of each possible value. We can see this function in action when we use <code>dnorm</code> to compute, say, the density value corresponding to <span class="math inline">\(y=1\)</span> in the <span class="math inline">\(Normal(\mu=0,\sigma=1)\)</span> distribution.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb30-1" data-line-number="1"><span class="co">## density:</span></a>
<a class="sourceLine" id="cb30-2" data-line-number="2"><span class="kw">dnorm</span>(<span class="dv">1</span>,<span class="dt">mean=</span><span class="dv">0</span>,<span class="dt">sd=</span><span class="dv">1</span>)</a></code></pre></div>
<pre><code>## [1] 0.242</code></pre>
<p>The quantity above is <em>not</em> the probability of observing 1 in this distribution. As mentioned earlier, probability in a continuous distribution is the area under the curve, and this area will always be zero at any point value. If we want to know the probability of obtaining values between an upper and lower bound <span class="math inline">\(b\)</span> and <span class="math inline">\(a\)</span>, i.e., <span class="math inline">\(P(a&lt;Y&lt;b)\)</span> where these are two distinct values, we must use the <code>pnorm</code> function. For example, the probability of observing a value between +2 and -2 in a Normal distribution with mean 0 and standard deviation 1 is:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" data-line-number="1"><span class="kw">pnorm</span>(<span class="dv">2</span>,<span class="dt">mean=</span><span class="dv">0</span>,<span class="dt">sd=</span><span class="dv">1</span>)<span class="op">-</span><span class="kw">pnorm</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dt">mean=</span><span class="dv">0</span>,<span class="dt">sd=</span><span class="dv">1</span>)</a></code></pre></div>
<pre><code>## [1] 0.9545</code></pre>
<p>Notice that the situation is different in discrete random variables. These have a probability mass function (PMF) associated with themâthe Binomial distribution that we saw earlier is an example. There, the PMF maps the possible <span class="math inline">\(y\)</span> values to the probabilities of those values occurring. That is why, in the Binomial distribution, the probability of observing exactly 2 successes when sampling from a <span class="math inline">\(Binomial(n=10,\theta=0.5)\)</span> can be computed using either <code>dbinom</code> or <code>pbinom</code>:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" data-line-number="1"><span class="kw">dbinom</span>(<span class="dv">2</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)</a></code></pre></div>
<pre><code>## [1] 0.04395</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb36-1" data-line-number="1"><span class="kw">pbinom</span>(<span class="dv">2</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)<span class="op">-</span><span class="kw">pbinom</span>(<span class="dv">1</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)</a></code></pre></div>
<pre><code>## [1] 0.04395</code></pre>
<p>In the second line of code above, we are computing the cumulative probability of observing two or less successes, minus the probability of observing one or less successes. This gives us the probability of observing exactly two successes. The <code>dbinom</code> gives us this same information.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>R will compute variance by dividing by <span class="math inline">\(n-1\)</span>, not <span class="math inline">\(n\)</span>; this is because dividing by <span class="math inline">\(n\)</span> gives a biased estimate. This is not an important detail for our purposes, and in any case for large <span class="math inline">\(n\)</span> it doesnât really matter whether one divides by <span class="math inline">\(n\)</span> or <span class="math inline">\(n-1\)</span>.<a href="continuous-random-variables-an-example-using-the-normal-distribution.html#fnref3" class="footnote-back">â©</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="discrete-random-variables-an-example-using-the-binomial-distribution.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/01-introductionBayesCogSci.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
