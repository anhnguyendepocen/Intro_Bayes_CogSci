<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.2 A discrete random variable with the Binomial distribution | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="1.2 A discrete random variable with the Binomial distribution | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://bookdown.org/yihui/bookdown/" />
  <meta property="og:image" content="https://bookdown.org/yihui/bookdown/images/cover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="rstudio/bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.2 A discrete random variable with the Binomial distribution | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://bookdown.org/yihui/bookdown/images/cover.jpg" />

<meta name="author" content="Shravan Vasishth, Bruno Nicenboim, and Daniel Schad" />


<meta name="date" content="2019-09-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introprob.html">
<link rel="next" href="references.html">
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Authoring Books with R Markdown</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>0.1</b> Prerequisites</a></li>
<li class="chapter" data-level="0.2" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i><b>0.2</b> Online materials</a></li>
<li class="chapter" data-level="0.3" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i><b>0.3</b> Software needed</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introprob.html"><a href="introprob.html"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="a-discrete-random-variable-with-the-binomial-distribution.html"><a href="a-discrete-random-variable-with-the-binomial-distribution.html"><i class="fa fa-check"></i><b>1.2</b> A discrete random variable with the Binomial distribution</a><ul>
<li class="chapter" data-level="1.2.1" data-path="a-discrete-random-variable-with-the-binomial-distribution.html"><a href="a-discrete-random-variable-with-the-binomial-distribution.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.2.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.2.2" data-path="a-discrete-random-variable-with-the-binomial-distribution.html"><a href="a-discrete-random-variable-with-the-binomial-distribution.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.2.2</b> What information does a probability distribution provide?</a></li>
<li class="chapter" data-level="1.2.3" data-path="a-discrete-random-variable-with-the-binomial-distribution.html"><a href="a-discrete-random-variable-with-the-binomial-distribution.html#sec:normalization"><i class="fa fa-check"></i><b>1.2.3</b> The normalization constant in pdfs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="a-discrete-random-variable-with-the-binomial-distribution" class="section level2">
<h2><span class="header-section-number">1.2</span> A discrete random variable with the Binomial distribution</h2>
<p>Imagine that our data consist of a sequence of 1’s and 0’s, where 1 represents “success”, and 0 represents “failure”. Assume for now that each 1 and 0 is generated independently from the others. This could be the kind of data we would get if we were asking participants to answer yes/no questions. We can repeatedly generate sequences of <span class="math inline">\(10\)</span> such data points in R (we will presently show how to do this):</p>
<pre><code>##  [1] 1 1 0 1 1 1 1 0 1 1</code></pre>
<pre><code>##  [1] 0 1 1 0 0 1 1 0 0 1</code></pre>
<pre><code>##  [1] 1 0 1 1 0 1 0 1 0 1</code></pre>
<p>These data are being generated by a discrete random variable <span class="math inline">\(Y\)</span> with a probability distribution <span class="math inline">\(p(Y)\)</span> called the Binomial distribution. For discrete random variable, the probability distribution <span class="math inline">\(p(y)\)</span> is called a <strong>probability mass function</strong> (PMF). The PMF defines the probability of each possible outcome. In the above example, with <span class="math inline">\(n=10\)</span> trials, there are 11 possible outcomes: <span class="math inline">\(0,\dots,10\)</span> successes. Which of these outcomes is most probable depends on a parameter in the Binomial distribution that represents the probability of success. , We will call this parameter <span class="math inline">\(\theta\)</span>. The left-hand side plot in Figure <a href="a-discrete-random-variable-with-the-binomial-distribution.html#fig:binomplot">1.1</a> shows an example of a Binomial PMF with <span class="math inline">\(10\)</span> trials and the parameter <span class="math inline">\(\theta\)</span> with value <span class="math inline">\(0.5\)</span>. Setting <span class="math inline">\(\theta\)</span> to 0.5 leads to a PMF where the most probable outcome is 5 successes out of 10. If we had set <span class="math inline">\(\theta\)</span> to, say 0.1, then the most probable outcome would be 1 success out of 10; and if we had set <span class="math inline">\(\theta\)</span> to 0.9, then the most probable outcome would be 9 successes out of 10.</p>
<div class="figure"><span id="fig:binomplot"></span>
<img src="bookdown_files/figure-html/binomplot-1.svg" alt="Probability mass functions of a binomial distribution assuming 10 trials, with 50%, 10%, and 90% probability of success." width="672" />
<p class="caption">
FIGURE 1.1: Probability mass functions of a binomial distribution assuming 10 trials, with 50%, 10%, and 90% probability of success.
</p>
</div>
<p>The probability mass function for the binomial is written as follows.</p>
<p><span class="math display">\[\begin{equation}
\hbox{Binomial}(n,k|\theta) = 
{n \choose k} \theta^{k} (1-\theta)^{n-k}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(n\)</span> represents the total number of trials, <span class="math inline">\(k\)</span> the number of successes, and <span class="math inline">\(\theta\)</span> the probability of success. The term <span class="math inline">\(n \choose k\)</span>, pronounced n-choose-k, represents the number of ways in which one can choose <span class="math inline">\(k\)</span> successes out of <span class="math inline">\(n\)</span> trials. For example, 1 success out of 10 can occur in 10 possible ways: the very first trial could be a 1, the secone trial could be a 1, etc.</p>
<div id="the-mean-and-variance-of-the-binomial-distribution" class="section level3">
<h3><span class="header-section-number">1.2.1</span> The mean and variance of the Binomial distribution</h3>
<p>It is possible to analytically compute the mean and variance of the PMF associated with the Binomial random variable <span class="math inline">\(Y\)</span>. Without getting into the details of how these are derived mathematically, we just state here that the mean of <span class="math inline">\(Y\)</span> (also called the expectation, conventionally written <span class="math inline">\(E[Y]\)</span>) and variance of <span class="math inline">\(Y\)</span> (written <span class="math inline">\(Var(Y)\)</span>) of a Binomial distribution with parameter <span class="math inline">\(\theta\)</span> and <span class="math inline">\(n\)</span> trials are <span class="math inline">\(E[Y] = n\theta\)</span> and <span class="math inline">\(Var(Y) = n\theta (1-\theta)\)</span>, respectively.</p>
<p>Of course, we always know <span class="math inline">\(n\)</span> (because we decide on the number of trials ourselves), but in real experimental situations we never know the true value of <span class="math inline">\(\theta\)</span>. But <span class="math inline">\(\theta\)</span> can be estimated from the data. From the observed data, we can compute the estimate of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\hat \theta=k/n\)</span>. The quantity <span class="math inline">\(\hat \theta\)</span> is the observed proportion of successes, and is called the <strong>maximum likelihood estimate</strong> of the true (but unknown mean). Once we have estimated <span class="math inline">\(\theta\)</span> in this way, we can also obtain an estimate (also a maximum likelihood estimate) of the variance by computing <span class="math inline">\(n\theta (1-\theta)\)</span>. These estimates are then used for statistical inference.</p>
<p>What does the term “maximum likelihood estimate” mean? The term <strong>likelihood</strong> refers to the value of the Binomial distribution function for a particular value of <span class="math inline">\(\theta\)</span>, once we have observed some data. For example, suppose you record <span class="math inline">\(n=10\)</span> trials, and observe <span class="math inline">\(k=7\)</span> successes. What is the probability of observing <span class="math inline">\(7\)</span> successes out of <span class="math inline">\(10\)</span>? We need the binomial distribution to compute this value:</p>
<p><span class="math display">\[\begin{equation}
\hbox{Binomial}(n=10,k=7|\theta) = 
{10 \choose 7} \theta^{7} (1-\theta)^{10-7}
\end{equation}\]</span></p>
<p>Once we have observed the data, both <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span> are fixed. The only variable in the above equation now is <span class="math inline">\(\theta\)</span>: the above function is now only dependent on the value of <span class="math inline">\(\theta\)</span>. Stated in this way, the function is called a <strong>likelihood function</strong> and can be written simply as a function of <span class="math inline">\(\theta\)</span>: <span class="math inline">\(\mathcal{L}(\theta)\)</span>. If we now plot this function for all possible values of <span class="math inline">\(\theta\)</span>, we get the plot shown in Figure <a href="a-discrete-random-variable-with-the-binomial-distribution.html#fig:binomlik">1.2</a>.</p>
<div class="figure"><span id="fig:binomlik"></span>
<img src="bookdown_files/figure-html/binomlik-1.svg" alt="The likelihood function for 7 successes out of 10." width="672" />
<p class="caption">
FIGURE 1.2: The likelihood function for 7 successes out of 10.
</p>
</div>
<p>What is important about this plot is that it shows that, given the data, the maximum point is at the point <span class="math inline">\(0.7\)</span>, which corresponds to the estimated mean using the formula shown above: <span class="math inline">\(k/n = 7/10\)</span>. Thus, the maximum likelihood estimate (MLE) gives us the most likely value that the parameter <span class="math inline">\(\theta\)</span> has <em>conditional on the data</em>. It is crucial to note here that the phrase ``most likely’’ here does not mean that the MLE necessarily gives us an accurate estimate of <span class="math inline">\(\theta\)</span>. For example, if we run our experiment for <span class="math inline">\(10\)</span> trials and get <span class="math inline">\(1\)</span> success out of <span class="math inline">\(10\)</span>, the MLE is <span class="math inline">\(0.10\)</span>. We could have happened to observe only one success out of ten even if the true <span class="math inline">\(\theta\)</span> were <span class="math inline">\(0.5\)</span>. The MLE would however give an accurate estimate of the true parameter as <span class="math inline">\(n\)</span> approaches infinity.</p>
</div>
<div id="what-information-does-a-probability-distribution-provide" class="section level3">
<h3><span class="header-section-number">1.2.2</span> What information does a probability distribution provide?</h3>
<p>What good is a probability mass function? Bayesian inference depends completely on computing properties of probability distributions of unknown parameters. It is therefore worth understanding what we can do with a probability distribution.</p>
<ul>
<li><strong>Compute the probability of a particular outcome (discrete case only)</strong>: The Binomial distribution shown in Figure <a href="a-discrete-random-variable-with-the-binomial-distribution.html#fig:binomplot">1.1</a> already shows the probability of each possible outcome under a different value for <span class="math inline">\(\theta\)</span>. In R, there is a built-in function that allows us to calculate the probability of <span class="math inline">\(k\)</span> successes out of <span class="math inline">\(n\)</span>, given a particular value of <span class="math inline">\(k\)</span> and the number of trials <span class="math inline">\(n\)</span> (these constitute our data), and given a particular value of <span class="math inline">\(\theta\)</span>. This is the <code>dnorm</code> function. For example, the probability of 5 successes out of 10 when <span class="math inline">\(\theta\)</span> is 0.5 is:</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="kw">dbinom</span>(<span class="dv">5</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)</a></code></pre></div>
<pre><code>## [1] 0.2461</code></pre>
<p>The probabilities of success when <span class="math inline">\(\theta\)</span> is 0.1 or 0.9 can be computed by replacing 0.5 above by each of these probabilities. Note that the probability of a particular outcome is only computable in the discrete case; in the case, this probability will always be zero (we discuss this in the next section).</p>
<ul>
<li><strong>Compute the cumulative probability of k or less (more) than k successes</strong>: Using the <code>dbinom</code> function, we can compute the cumulative probability of obtaining 1 or less, 2 or less successes etc. This is done through a simple summation procedure:</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="co">## the cumulative probability of obtaining</span></a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="co">## 0, 1, or 2 successes out of 10,</span></a>
<a class="sourceLine" id="cb7-3" data-line-number="3"><span class="co">## with theta=0.5:</span></a>
<a class="sourceLine" id="cb7-4" data-line-number="4"><span class="kw">dbinom</span>(<span class="dv">0</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)<span class="op">+</span><span class="kw">dbinom</span>(<span class="dv">1</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)<span class="op">+</span><span class="kw">dbinom</span>(<span class="dv">2</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)</a></code></pre></div>
<pre><code>## [1] 0.05469</code></pre>
<p>Mathematically, we would write the above as:</p>
<p><span class="math display">\[\begin{equation}

\end{equationn}

R has a built-in function called ```pbinom``` that does this summation for us.  If we want to know the probability of 2 or less successes as in the above example, we can write:


```r
pbinom(2,size=10,prob=0.5,lower.tail=TRUE)
```

```
## [1] 0.05469
```

The specification ```lower.tail=TRUE``` ensures that the summation goes from 2 to numbers smaller than 2 (which lie in the lower tail of the distribution in Figure \@ref(fig:binomplot)). If we wanted to know what the probability is of obtaining 2 or more successes out of 10, we can set lower.tail to FALSE:


```r
pbinom(2,size=10,prob=0.5,lower.tail=FALSE)
```

```
## [1] 0.9453
```

The cumulative distribution function or CDF can be plotted by computing the cumulative probabilities for any value $k$ or less than $k$, where $k$ ranges from $0$ to $10$ in our running example. The CDF is shown in Figure \@ref(fig:binomcdf).


&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;bookdown_files/figure-html/binomcdf-1.svg&quot; alt=&quot;The cumulative distribution function for a binomial distribution assuming 10 trials, with 50% probability of success.&quot; width=&quot;672&quot; /&gt;
&lt;p class=&quot;caption&quot;&gt;(\#fig:binomcdf)The cumulative distribution function for a binomial distribution assuming 10 trials, with 50% probability of success.&lt;/p&gt;
&lt;/div&gt;

* **The inverse of the cumulative distribution function (the quantile function)**: We can also find out the value of the variable $k$ (the quantile) such that the probability of obtaining $k$ or less than $k$ successes is some probability value $p$. If we switch the x and y axes of Figure \@ref(fig:binomcdf), we obtain another very useful function, the inverse CDF. This is shown in Figure \@ref(fig:binominvcdf) below.

&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;bookdown_files/figure-html/binominvcdf-1.svg&quot; alt=&quot;The inverse cumulative distribution function for a binomial distribution assuming 10 trials, with 50% probability of success.&quot; width=&quot;672&quot; /&gt;
&lt;p class=&quot;caption&quot;&gt;(\#fig:binominvcdf)The inverse cumulative distribution function for a binomial distribution assuming 10 trials, with 50% probability of success.&lt;/p&gt;
&lt;/div&gt;

The inverse of the CDF (known as the quantile function in R because it returns the quantile, the value k) is available in R as the function ```qbinom```. The usage is as follows: to find out what the value $k$ of the outcome is such that the probability of obtaining $k$ or less successes is $0.37$, type:


```r
qbinom(0.37,size=10,prob=0.5)
```

```
## [1] 4
```

You can confirm that the above answer is correct by using the CDF to find out the probability that k is less than or equal to 4:


```r
pbinom(4,size=10,prob=0.5)
```

```
## [1] 0.377
```

&lt;!-- to-do: explain why qbinom(0.77 gives 5 as an answer and not 4)--&gt;

* **Generating random data from a $\hbox{Binomial}(n,p)$ distribution**: We can generate random simulated data from a Binomial distribution by specifying the number of trials and the probability of success $\theta$. In R, we do this as follows:


```r
rbinom(10,size=1,prob=0.5)
```

```
##  [1] 1 1 0 1 0 1 0 0 1 1
```

The above code generates a sequences of 1&#39;s and 0&#39;s. Repeatedly run the above code; you will get different sequences each time. For each generated sequence, one can calculate the number of successes by just summing up the vector, or computing its mean and multiplying by the number of trials, here 10:



```r
y&lt;-rbinom(10,size=1,prob=0.5)
mean(y)*10 ; sum(y)
```

```
## [1] 6
```

```
## [1] 6
```

### The maximum likelihood estimates of the mean and the variance

The probability distribution associated with the random variable $Y$ is the binomial distribution mentioned above: ${n \choose k} \theta^k (1-\theta)^{n-k}$.   


A random variable $X$ is a function $X : S \rightarrow \mathbb{R}$ that associates to each outcome
$\omega \in S$ exactly one number $X(\omega) = x$.

$S_X$ is all the $x$&#39;s (all the possible values of X, the support of X). I.e., $x \in S_X$. We can also sloppily write $X \in S_X$. 

Good example: number of coin tosses till H

\begin{itemize}
  \item $X: \omega \rightarrow x$
    \item $\omega$: H, TH, TTH,\dots (infinite)
    \item $x=0,1,2,\dots; x \in S_X$
\end{itemize}

Every discrete (continuous) random variable X has associated with it a \textbf{probability mass (distribution)  function (pmf, pdf)}. I.e., PMF is used for discrete distribu tions and PDF for continuous. (I will sometimes use lower case for pdf and sometimes upper case. Some books use pdf for both discrete and continuous distributions.)

\begin{equation}
p_X : S_X \rightarrow [0, 1] 
\end{equation}\]</span></p>
<p>defined by</p>
<p><span class="math display">\[\begin{equation}
p_X(x) = P(X(\omega) = x), x \in S_X
 \end{equation}\]</span></p>
<p>[: Books sometimes abuse notation by overloading the meaning of <span class="math inline">\(X\)</span>. They usually have: <span class="math inline">\(p_X(x) = P(X = x), x \in S_X\)</span>]</p>
<p>Probability density functions (continuous case) or probability mass functions (discrete case) are functions that assign probabilities or relative frequencies to all events in a sample space.</p>
<p>The expression</p>
<p><span class="math display">\[\begin{equation}
 X \sim f(\cdot)
\end{equation}\]</span></p>
<p>
means that the random variable <span class="math inline">\(X\)</span> has pdf/pmf <span class="math inline">\(f(\cdot)\)</span>.
For example, if we say that <span class="math inline">\(X\sim N(\mu,\sigma)\)</span>, we are assuming that the pdf is</p>
<p><span class="math display">\[\begin{equation}
f(x)= \frac{1}{\sqrt{2\pi \sigma^2}} \exp[-\frac{(x-\mu)^2}{2\sigma^2}]
\end{equation}\]</span></p>
<p>We also need a  or cdf because, in the continuous case, P(X=some point value) is zero and we need a way to talk about P(X in a specific range). cdfs serve that purpose.</p>
<p>In the continuous case, the cdf or distribution function is defined as:</p>
<p><span class="math display">\[\begin{equation}
P(X&lt;x) = F(X&lt;x) =\int_{-\infty}^{X} f(x)\, dx
\end{equation}\]</span></p>
</div>
<div id="sec:normalization" class="section level3">
<h3><span class="header-section-number">1.2.3</span> The normalization constant in pdfs</h3>
<p>Almost any function can be a pdf as long as the area under the curve sums to 1 over the sample space. Here is an example of a function whose area under the curve doesn’t sum to 1:</p>
<p><span class="math display">\[\begin{equation}
f(x)=\exp[-\frac{(x-\mu)^2}{2 \sigma^2}]
\end{equation}\]</span></p>
<p>This is the ``kernel’’ of the normal pdf, and it doesn’t sum to 1. We can show that quickly by writing a function in R that expresses this kernel, and then summing up the area under the curve by <strong>integrating</strong> the function in R, from -Infinity to +Infinity.</p>
<p>In what is shown below, integrating the function f(x) is written in mathematics as <span class="math inline">\(\int_{a}^{b} f(x) dx\)</span>, and simply means that we sum up the area under the continuous function between the ranges a and b.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="co">## define function:</span></a>
<a class="sourceLine" id="cb9-2" data-line-number="2">normkernel&lt;-<span class="cf">function</span>(x,<span class="dt">mu=</span><span class="dv">0</span>,<span class="dt">sigma=</span><span class="dv">1</span>){</a>
<a class="sourceLine" id="cb9-3" data-line-number="3">  <span class="kw">exp</span>((<span class="op">-</span>(x<span class="op">-</span>mu)<span class="op">^</span><span class="dv">2</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>(sigma<span class="op">^</span><span class="dv">2</span>))))</a>
<a class="sourceLine" id="cb9-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb9-5" data-line-number="5"></a>
<a class="sourceLine" id="cb9-6" data-line-number="6"><span class="co">## plot kernel density function:</span></a>
<a class="sourceLine" id="cb9-7" data-line-number="7"><span class="kw">plot</span>(<span class="cf">function</span>(x) <span class="kw">normkernel</span>(x), <span class="dv">-3</span>, <span class="dv">3</span>,</a>
<a class="sourceLine" id="cb9-8" data-line-number="8">      <span class="dt">main =</span> <span class="st">&quot;Normal density&quot;</span>,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</a>
<a class="sourceLine" id="cb9-9" data-line-number="9">              <span class="dt">ylab=</span><span class="st">&quot;density&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;X&quot;</span>)</a></code></pre></div>
<p><img src="bookdown_files/figure-html/normkernel-1.svg" width="672" /></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="co">## compute area under the curve</span></a>
<a class="sourceLine" id="cb10-2" data-line-number="2"><span class="co">## the area under the curve is not equal to 1:</span></a>
<a class="sourceLine" id="cb10-3" data-line-number="3"><span class="kw">integrate</span>(normkernel,<span class="dt">lower=</span><span class="op">-</span><span class="ot">Inf</span>,<span class="dt">upper=</span><span class="ot">Inf</span>)</a></code></pre></div>
<pre><code>## 2.507 with absolute error &lt; 0.00023</code></pre>
<p>So, here, <span class="math inline">\(\int_{-\infty}^{\infty} f(x) dx=2.51\)</span>.</p>
<p>Adding a normalizing constant, <span class="math inline">\(\frac{1}{\sqrt{2\pi\sigma^2}}\)</span>, makes the above kernel density a pdf.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1">norm&lt;-<span class="cf">function</span>(x,<span class="dt">mu=</span><span class="dv">0</span>,<span class="dt">sigma=</span><span class="dv">1</span>){</a>
<a class="sourceLine" id="cb12-2" data-line-number="2">  (<span class="dv">1</span><span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span>(sigma<span class="op">^</span><span class="dv">2</span>))) <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>((<span class="op">-</span>(x<span class="op">-</span>mu)<span class="op">^</span><span class="dv">2</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>(sigma<span class="op">^</span><span class="dv">2</span>))))</a>
<a class="sourceLine" id="cb12-3" data-line-number="3">}</a>
<a class="sourceLine" id="cb12-4" data-line-number="4"></a>
<a class="sourceLine" id="cb12-5" data-line-number="5"><span class="kw">plot</span>(<span class="cf">function</span>(x) <span class="kw">norm</span>(x), <span class="dv">-3</span>, <span class="dv">3</span>,</a>
<a class="sourceLine" id="cb12-6" data-line-number="6">      <span class="dt">main =</span> <span class="st">&quot;Normal density&quot;</span>,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</a>
<a class="sourceLine" id="cb12-7" data-line-number="7">              <span class="dt">ylab=</span><span class="st">&quot;density&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;X&quot;</span>)</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-5-1.svg" width="672" /></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="co">### area under the curve sums to 1:</span></a>
<a class="sourceLine" id="cb13-2" data-line-number="2"><span class="kw">integrate</span>(norm,<span class="dt">lower=</span><span class="op">-</span><span class="ot">Inf</span>,<span class="dt">upper=</span><span class="ot">Inf</span>)</a></code></pre></div>
<pre><code>## 1 with absolute error &lt; 9.4e-05</code></pre>
<p>Now, <span class="math inline">\(\int_{-\infty}^{\infty} f(x) dx=1\)</span>.</p>

</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="introprob.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/01-introductionBayesCogSci.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
